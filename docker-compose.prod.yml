services:
  db:
    image: postgres:16-alpine
    restart: unless-stopped
    env_file:
      - .env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  app:
    build: .
    restart: unless-stopped
    env_file:
      - .env
    environment:
      REDIS_URL: ${REDIS_URL:-redis://redis:6379/0}
      AUTH_RATE_LIMIT_BACKEND: ${AUTH_RATE_LIMIT_BACKEND:-redis}
      AUTH_RATE_LIMIT_REDIS_NAMESPACE: ${AUTH_RATE_LIMIT_REDIS_NAMESPACE:-auth_rate_limit}
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    ports:
      - "8000:8000"
    healthcheck:
      test:
        [
          "CMD",
          "python",
          "-c",
          "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')",
        ]
      interval: 15s
      timeout: 5s
      retries: 5

  outbox-worker:
    build: .
    restart: unless-stopped
    env_file:
      - .env
    environment:
      OUTBOX_WORKER_MODE: loop
      OUTBOX_WORKER_POLL_SECONDS: 10
      REDIS_URL: ${REDIS_URL:-redis://redis:6379/0}
      AUTH_RATE_LIMIT_BACKEND: ${AUTH_RATE_LIMIT_BACKEND:-redis}
      AUTH_RATE_LIMIT_REDIS_NAMESPACE: ${AUTH_RATE_LIMIT_REDIS_NAMESPACE:-auth_rate_limit}
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    command: python -m app.workers.outbox_notifications_worker

  prometheus:
    image: prom/prometheus:v3.5.0
    restart: unless-stopped
    depends_on:
      app:
        condition: service_started
      alertmanager:
        condition: service_started
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
    volumes:
      - ./ops/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./ops/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"

  alertmanager:
    image: prom/alertmanager:v0.28.1
    restart: unless-stopped
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
    volumes:
      - ./ops/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    ports:
      - "9093:9093"

  grafana:
    image: grafana/grafana:11.3.0
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_started
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./ops/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./ops/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3000:3000"

volumes:
  pg_data:
  prometheus_data:
  alertmanager_data:
  grafana_data:
